{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab:  Model Validation With Gradient Boosting\n",
    "\n",
    "Welcome to this evening's lab!  It's going to be a fun one.  For today's class, we're going to try and take a crack at model building in a wholistic way.  \n",
    "\n",
    "Specifically, we're going to try and do three different things:\n",
    "\n",
    " - Try out different versions of our data, and use our validation scores to see if something was an improvement or not\n",
    " - We're going to adjust model parameters to try and adjust our results to help curb overfitting\n",
    " - We're going to try and find model parameters that maximize our score for our dataset\n",
    " \n",
    "The idea is that we'll be able to do a mini-walkthrough to test what it's like to build and validate a model and try and improve our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Using the suggestions from the homework prompt given previously, try and add 3-4 different features ( columns ) to your data, and use your validation score to determine if they improved your results.  \n",
    "\n",
    "This is meant to be open ended, and to allow you a chance to re-discover material from previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import category_encoders as ce\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "df = pd.read_csv('../../data/restaurants.csv', parse_dates=['visit_date'])\n",
    "df.drop('calendar_date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some functions that we can reuse\n",
    "def create_val_splits(df, val_units=15, return_val=False):\n",
    "    \"\"\"Function that will take in a dataset and split it up into training, validation, and test sets\"\"\"\n",
    "    # split into training, validation, and test sets\n",
    "    df = df.drop('visit_date', axis=1)\n",
    "    train = df.groupby('id').apply(lambda x: x.iloc[:-val_units]).reset_index(drop=True)\n",
    "    test  = df.groupby('id').apply(lambda x: x.iloc[-val_units:]).reset_index(drop=True)\n",
    "    \n",
    "    if return_val:\n",
    "        val   = train.groupby('id').apply(lambda x: x.iloc[-val_units:]).reset_index(drop=True)\n",
    "        train = train.groupby('id').apply(lambda x: x.iloc[:-val_units]).reset_index(drop=True)\n",
    "        return train, val, test\n",
    "    else:\n",
    "        return train, test\n",
    "    \n",
    "def denote_null_values(df):\n",
    "    \"\"\"Denotes whether or not there are null values or not\"\"\"\n",
    "    empty_cols_query = df.isnull().sum() > 0\n",
    "    empty_df_cols = df.loc[:, empty_cols_query].columns.tolist()\n",
    "    for col in empty_df_cols:\n",
    "        col_name = f\"{col}_missing\"\n",
    "        df[col_name] = pd.isnull(df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4587706012625258"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll start of with an initial model fitting\n",
    "# fill in missing values\n",
    "df = denote_null_values(df)\n",
    "df = df.fillna(0)\n",
    "\n",
    "# and do an initial fitting\n",
    "train, val, test = create_val_splits(df, return_val=True)\n",
    "\n",
    "# create a pipeline, and get our model score\n",
    "pipe = make_pipeline(ce.TargetEncoder(), GradientBoostingRegressor())\n",
    "\n",
    "# split into X & y\n",
    "X_train, y_train = train.drop('visitors', axis=1), train['visitors']\n",
    "X_val, y_val = val.drop('visitors', axis=1), val['visitors']\n",
    "\n",
    "# fit & score\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our current baseline to work with.  Now we'll add some features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5162597129609126"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll do month & previous values, like we had before\n",
    "# sort by date -- just to make sure\n",
    "df.sort_values(by=['id', 'visit_date'], ascending=True, inplace=True)\n",
    "df['month'] = df['visit_date'].dt.month\n",
    "df['yesterday'] = df.groupby('id').apply(lambda x: x['visitors'].shift()).values\n",
    "\n",
    "# denote the missing values, and fill\n",
    "df = denote_null_values(df)\n",
    "df = df.bfill()\n",
    "\n",
    "# split the data, fit & score\n",
    "train, val, test = create_val_splits(df, return_val=True)\n",
    "X_train, y_train = train.drop('visitors', axis=1), train['visitors']\n",
    "X_val, y_val = val.drop('visitors', axis=1), val['visitors']\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better.  Let's try doing this a few more times to see if we can keep making improvements.  Let's try our hand at some summary statistics -- comparing observations to some larger category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try creating a grouping that captures typical attendance for a city + day of week\n",
    "# we'll first create training, validation, & test sets\n",
    "df['city'] = df.area.str.split('-').str[0]\n",
    "train, val, test = create_val_splits(df, return_val=True)\n",
    "\n",
    "# and get our grouping -- notice we are doing this on the TEST set, and applying this to the validation set\n",
    "city_avgs = train.groupby(['city', 'genre', 'day_of_week'])[['visitors']].mean().rename({'visitors': 'city_genre_day_visits'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.531904270570581"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and merge it back in\n",
    "train = train.merge(city_avgs, left_on=['city', 'genre', 'day_of_week'], right_index=True, how='left')\n",
    "val   = val.merge(city_avgs, left_on=['city', 'genre', 'day_of_week'], right_index=True, how='left')\n",
    "\n",
    "# create our splits\n",
    "X_train, y_train = train.drop('visitors', axis=1), train['visitors']\n",
    "X_val, y_val = val.drop('visitors', axis=1), val['visitors']\n",
    "\n",
    "# re-initialize the pipe to include the city column\n",
    "target_encoder = ce.TargetEncoder()\n",
    "pipe = make_pipeline(target_encoder, GradientBoostingRegressor())\n",
    "\n",
    "# and fit + score\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still getting better, so we'll keep things.  Notice that we haven't added this column to the entire dataset yet.......let's add it back into the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge things back into the entire dataset so it's a permanent column\n",
    "df = df.merge(city_avgs, left_on=['city', 'genre', 'day_of_week'], right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try adding in some moving average values to see if this helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25\n",
       "1    32\n",
       "2    29\n",
       "Name: visitors, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['visitors'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ce.TargetEncoder(), GradientBoostingRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_vals = df.groupby(['genre', 'day_of_week'])[['visitors']].mean().rename({'visitors': 'genre_mean'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2 = df2.merge(genre_vals, left_on=['genre', 'day_of_week'], right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['diff'] = (df2['visitors'] - df2['genre_mean']) / df2['genre_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['genre_mean'] = df2.groupby('id').apply(lambda x: x['genre_mean'].shift()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9873638200164768"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(df2.drop(['visitors', 'visit_date'], axis=1), df2['visitors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>visitors</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>holiday</th>\n",
       "      <th>genre</th>\n",
       "      <th>area</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>reserve_visitors</th>\n",
       "      <th>city</th>\n",
       "      <th>genre_mean</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>25</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tōkyō</td>\n",
       "      <td>16.984222</td>\n",
       "      <td>0.471954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>32</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tōkyō</td>\n",
       "      <td>17.049499</td>\n",
       "      <td>0.876888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>29</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tōkyō</td>\n",
       "      <td>21.703760</td>\n",
       "      <td>0.336174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>22</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>0</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tōkyō</td>\n",
       "      <td>24.718941</td>\n",
       "      <td>-0.109994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>6</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tōkyō</td>\n",
       "      <td>14.543439</td>\n",
       "      <td>-0.587443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id visit_date  visitors day_of_week  holiday       genre  \\\n",
       "0  air_ba937bf13d40fb24 2016-01-13        25   Wednesday        0  Dining bar   \n",
       "1  air_ba937bf13d40fb24 2016-01-14        32    Thursday        0  Dining bar   \n",
       "2  air_ba937bf13d40fb24 2016-01-15        29      Friday        0  Dining bar   \n",
       "3  air_ba937bf13d40fb24 2016-01-16        22    Saturday        0  Dining bar   \n",
       "4  air_ba937bf13d40fb24 2016-01-18         6      Monday        0  Dining bar   \n",
       "\n",
       "                           area   latitude   longitude  reserve_visitors  \\\n",
       "0  Tōkyō-to Minato-ku Shibakōen  35.658068  139.751599               NaN   \n",
       "1  Tōkyō-to Minato-ku Shibakōen  35.658068  139.751599               NaN   \n",
       "2  Tōkyō-to Minato-ku Shibakōen  35.658068  139.751599               NaN   \n",
       "3  Tōkyō-to Minato-ku Shibakōen  35.658068  139.751599               NaN   \n",
       "4  Tōkyō-to Minato-ku Shibakōen  35.658068  139.751599               NaN   \n",
       "\n",
       "    city  genre_mean      diff  \n",
       "0  Tōkyō   16.984222  0.471954  \n",
       "1  Tōkyō   17.049499  0.876888  \n",
       "2  Tōkyō   21.703760  0.336174  \n",
       "3  Tōkyō   24.718941 -0.109994  \n",
       "4  Tōkyō   14.543439 -0.587443  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>visitors</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>holiday</th>\n",
       "      <th>genre</th>\n",
       "      <th>area</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>reserve_visitors</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>25</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tōkyō</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>32</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tōkyō</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>29</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tōkyō</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>22</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>0</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tōkyō</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>6</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tōkyō</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id visit_date  visitors day_of_week  holiday       genre  \\\n",
       "0  air_ba937bf13d40fb24 2016-01-13        25   Wednesday        0  Dining bar   \n",
       "1  air_ba937bf13d40fb24 2016-01-14        32    Thursday        0  Dining bar   \n",
       "2  air_ba937bf13d40fb24 2016-01-15        29      Friday        0  Dining bar   \n",
       "3  air_ba937bf13d40fb24 2016-01-16        22    Saturday        0  Dining bar   \n",
       "4  air_ba937bf13d40fb24 2016-01-18         6      Monday        0  Dining bar   \n",
       "\n",
       "                           area   latitude   longitude  reserve_visitors  \\\n",
       "0  Tōkyō-to Minato-ku Shibakōen  35.658068  139.751599               NaN   \n",
       "1  Tōkyō-to Minato-ku Shibakōen  35.658068  139.751599               NaN   \n",
       "2  Tōkyō-to Minato-ku Shibakōen  35.658068  139.751599               NaN   \n",
       "3  Tōkyō-to Minato-ku Shibakōen  35.658068  139.751599               NaN   \n",
       "4  Tōkyō-to Minato-ku Shibakōen  35.658068  139.751599               NaN   \n",
       "\n",
       "    city  \n",
       "0  Tōkyō  \n",
       "1  Tōkyō  \n",
       "2  Tōkyō  \n",
       "3  Tōkyō  \n",
       "4  Tōkyō  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll create window statistics for 7, 30, & 60 days -- notice the use of shift() here\n",
    "df['7DayAvg']  = df.groupby('id').apply(lambda x: x['visitors'].rolling(7).mean().shift()).values\n",
    "df['30DayAvg'] = df.groupby('id').apply(lambda x: x['visitors'].rolling(30).mean().shift()).values\n",
    "df['60DayAvg'] = df.groupby('id').apply(lambda x: x['visitors'].rolling(60).mean().shift()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.556299886924623"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note missing vals, backfill, and split the data\n",
    "df = denote_null_values(df)\n",
    "df = df.bfill()\n",
    "\n",
    "train, val, test = create_val_splits(df, return_val=True)\n",
    "\n",
    "X_train, y_train = train.drop('visitors', axis=1), train['visitors']\n",
    "X_val, y_val     = val.drop('visitors', axis=1), val['visitors']\n",
    "\n",
    "# fit and score\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good......all of these additions made modest, but real improvements to our out-of-sample scores, so we'll keep them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Try and reduce overfitting in your model, if it's persistent.  Ideally, you want your in-sample and out-of-sample scores to be about the same, or at least increasing or decreasing in proportional amounts.  \n",
    "\n",
    "The idea here is two-fold:  see if you can narrow the gap between in-sample and out-of-sample results (using training & validation sets), while simultaneously **not** decreasing your model scores (or at least not by very much).  The idea being that the closer these two are, the more reliable your results are likely to be.\n",
    "\n",
    "Some knobs you can turn:\n",
    " - `min_samples_leaf`: parameter in the category encoder that determines what cutoff point you can use for using the local vs. global average for the category\n",
    " - `subsample`: parameter in gbm that determines what fraction of your dataset to use at each boosting round.  This both reduces training time and makes each fitting round less related to the other\n",
    " - `max_features`: what portion of columns to use at each split.  This is very similar in purpose to `subsample`, but randomizes data at each split, vs. each round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.TargetEncoder().set_params(min_samples_leaf=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.973761245180636"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['visitors'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cols': None,\n",
       " 'drop_invariant': False,\n",
       " 'handle_missing': 'value',\n",
       " 'handle_unknown': 'value',\n",
       " 'min_samples_leaf': 20,\n",
       " 'return_df': True,\n",
       " 'smoothing': 1.0,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5201554721545789, 0.556299886924623)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "# we'll first see how our model is overfitting\n",
    "pipe.score(X_train, y_train), pipe.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results......are good.  Our out-sample-results are better than in-sample, so we won't complain.  However, these results are not guaranteed, and if there was a gap of more than 3-5% between in-sample + out-of-sample, we might try and tweak these to improve our results.\n",
    "\n",
    "However, to improve fitting times, let's see if we can tweak some GBM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5220539599716651, 0.5525071701013251)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll try a GBM that only uses 70% of columns at each split\n",
    "pipe = make_pipeline(ce.TargetEncoder(), GradientBoostingRegressor(max_features=0.7))\n",
    "\n",
    "# and see our results\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seemed like it had the intended effect -- faster fitting times, no appreciable change in scores, so we'll keep it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Using the results that gave you the best answer from above, try now to find model parameters that maximize information extraction.  The three main ones are:\n",
    "\n",
    " - `n_estimators`:  how many boosting rounds to use\n",
    " - `learning_rate`: how much shrinkage to use at each update (keep this from .05 to .2)\n",
    " - `max_depth`: how deep each tree in your model goes\n",
    " \n",
    " **important:** fitting these things could take a looooong time.  We don't have all night.  So don't try and make this exhaustive, just try doing a little bit of parameter exploration to see if you can see in what directions to push model parameters to improve your results.  \n",
    " \n",
    " Note your validation score before proceeding to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with parameters:  n_estimators - 100, learning_rate - 0.05, max_depth - 3\n",
      "Out-of-sample score: 0.5440895818411422\n",
      "Fitting model with parameters:  n_estimators - 100, learning_rate - 0.05, max_depth - 4\n",
      "Out-of-sample score: 0.5529674736267103\n",
      "Fitting model with parameters:  n_estimators - 100, learning_rate - 0.05, max_depth - 5\n",
      "Out-of-sample score: 0.5617630217802974\n",
      "Fitting model with parameters:  n_estimators - 100, learning_rate - 0.05, max_depth - 6\n",
      "Out-of-sample score: 0.571682201805663\n",
      "Fitting model with parameters:  n_estimators - 100, learning_rate - 0.1, max_depth - 3\n",
      "Out-of-sample score: 0.5528157678655643\n",
      "Fitting model with parameters:  n_estimators - 100, learning_rate - 0.1, max_depth - 4\n",
      "Out-of-sample score: 0.5637513461215058\n",
      "Fitting model with parameters:  n_estimators - 100, learning_rate - 0.1, max_depth - 5\n",
      "Out-of-sample score: 0.5706105603270158\n",
      "Fitting model with parameters:  n_estimators - 100, learning_rate - 0.1, max_depth - 6\n",
      "Out-of-sample score: 0.5810785397989502\n",
      "Fitting model with parameters:  n_estimators - 100, learning_rate - 0.2, max_depth - 3\n",
      "Out-of-sample score: 0.5643872181239744\n",
      "Fitting model with parameters:  n_estimators - 100, learning_rate - 0.2, max_depth - 4\n",
      "Out-of-sample score: 0.5737809223409045\n",
      "Fitting model with parameters:  n_estimators - 100, learning_rate - 0.2, max_depth - 5\n",
      "Out-of-sample score: 0.5703521909161773\n",
      "Fitting model with parameters:  n_estimators - 100, learning_rate - 0.2, max_depth - 6\n",
      "Out-of-sample score: 0.5751093569269111\n",
      "Fitting model with parameters:  n_estimators - 250, learning_rate - 0.05, max_depth - 3\n",
      "Out-of-sample score: 0.558411458170369\n",
      "Fitting model with parameters:  n_estimators - 250, learning_rate - 0.05, max_depth - 4\n",
      "Out-of-sample score: 0.5694452312692436\n",
      "Fitting model with parameters:  n_estimators - 250, learning_rate - 0.05, max_depth - 5\n",
      "Out-of-sample score: 0.5770560832440629\n",
      "Fitting model with parameters:  n_estimators - 250, learning_rate - 0.05, max_depth - 6\n",
      "Out-of-sample score: 0.5820359997393033\n",
      "Fitting model with parameters:  n_estimators - 250, learning_rate - 0.1, max_depth - 3\n",
      "Out-of-sample score: 0.5675776259514302\n",
      "Fitting model with parameters:  n_estimators - 250, learning_rate - 0.1, max_depth - 4\n",
      "Out-of-sample score: 0.5749435464826801\n",
      "Fitting model with parameters:  n_estimators - 250, learning_rate - 0.1, max_depth - 5\n",
      "Out-of-sample score: 0.5793773197213687\n",
      "Fitting model with parameters:  n_estimators - 250, learning_rate - 0.1, max_depth - 6\n",
      "Out-of-sample score: 0.5830020419768792\n",
      "Fitting model with parameters:  n_estimators - 250, learning_rate - 0.2, max_depth - 3\n",
      "Out-of-sample score: 0.5659177579068222\n",
      "Fitting model with parameters:  n_estimators - 250, learning_rate - 0.2, max_depth - 4\n",
      "Out-of-sample score: 0.5653264777593057\n",
      "Fitting model with parameters:  n_estimators - 250, learning_rate - 0.2, max_depth - 5\n",
      "Out-of-sample score: 0.5810044472186819\n",
      "Fitting model with parameters:  n_estimators - 250, learning_rate - 0.2, max_depth - 6\n",
      "Out-of-sample score: 0.588489928887224\n",
      "Fitting model with parameters:  n_estimators - 500, learning_rate - 0.05, max_depth - 3\n",
      "Out-of-sample score: 0.5685907919907632\n",
      "Fitting model with parameters:  n_estimators - 500, learning_rate - 0.05, max_depth - 4\n",
      "Out-of-sample score: 0.5751045046043932\n",
      "Fitting model with parameters:  n_estimators - 500, learning_rate - 0.05, max_depth - 5\n",
      "Out-of-sample score: 0.5808558802164182\n",
      "Fitting model with parameters:  n_estimators - 500, learning_rate - 0.05, max_depth - 6\n",
      "Out-of-sample score: 0.5893105805148674\n",
      "Fitting model with parameters:  n_estimators - 500, learning_rate - 0.1, max_depth - 3\n",
      "Out-of-sample score: 0.5708618292034436\n",
      "Fitting model with parameters:  n_estimators - 500, learning_rate - 0.1, max_depth - 4\n",
      "Out-of-sample score: 0.5776152512649515\n",
      "Fitting model with parameters:  n_estimators - 500, learning_rate - 0.1, max_depth - 5\n",
      "Out-of-sample score: 0.5844500750058816\n",
      "Fitting model with parameters:  n_estimators - 500, learning_rate - 0.1, max_depth - 6\n",
      "Out-of-sample score: 0.5894596984784964\n",
      "Fitting model with parameters:  n_estimators - 500, learning_rate - 0.2, max_depth - 3\n",
      "Out-of-sample score: 0.5687965559673833\n",
      "Fitting model with parameters:  n_estimators - 500, learning_rate - 0.2, max_depth - 4\n",
      "Out-of-sample score: 0.5834427609811069\n",
      "Fitting model with parameters:  n_estimators - 500, learning_rate - 0.2, max_depth - 5\n",
      "Out-of-sample score: 0.5725966600988234\n",
      "Fitting model with parameters:  n_estimators - 500, learning_rate - 0.2, max_depth - 6\n",
      "Out-of-sample score: 0.5818610007813463\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# setup model parameters\n",
    "n_estimators  = [100, 250, 500]\n",
    "learning_rate = [.05, .1, .2]\n",
    "max_depth     = [3, 4, 5, 6]\n",
    "cv_scores     = []\n",
    "\n",
    "# and cycle through our model parameters\n",
    "for estimators in n_estimators:\n",
    "    for rate in learning_rate:\n",
    "        for depth in max_depth:\n",
    "            print(f\"Fitting model with parameters:  n_estimators - {estimators}, learning_rate - {rate}, max_depth - {depth}\")\n",
    "            mod   = GradientBoostingRegressor(n_estimators=estimators, learning_rate=rate, max_depth=depth, max_features=0.6)\n",
    "            pipe  = make_pipeline(ce.TargetEncoder(), mod)\n",
    "            pipe.fit(X_train, y_train)\n",
    "            score = pipe.score(X_val, y_val)\n",
    "            print(f\"Out-of-sample score: {score}\")\n",
    "            cv_scores.append((score, estimators, rate, depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Take the best version of your model & your data, and fit it on **all** of your training + validation data.  The idea is that now that we've found the best version of what we have to work with, we want to give it as much training samples as possible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:47:47] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('targetencoder',\n",
       "                 TargetEncoder(cols=['id', 'day_of_week', 'genre', 'area',\n",
       "                                     'city'],\n",
       "                               drop_invariant=False, handle_missing='value',\n",
       "                               handle_unknown='value', min_samples_leaf=1,\n",
       "                               return_df=True, smoothing=1.0, verbose=0)),\n",
       "                ('xgbregressor',\n",
       "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                              colsample_bylevel=1, colsample_bynode=1,\n",
       "                              colsample_bytree=1, gamma=0,\n",
       "                              importance_type='gain', learning_rate=0.1,\n",
       "                              max_delta_step=0, max_depth=6, max_features=0.6,\n",
       "                              min_child_weight=1, missing=None,\n",
       "                              n_estimators=500, n_jobs=1, nthread=None,\n",
       "                              objective='reg:linear', random_state=0,\n",
       "                              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                              seed=None, silent=None, subsample=1,\n",
       "                              verbosity=1))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "pipe[1].set_params(n_estimators=500, max_depth=6, learning_rate=.1)\n",
    "train, test = create_val_splits(df)\n",
    "\n",
    "X_train, y_train = train.drop('visitors', axis=1), train['visitors']\n",
    "X_test, y_test   = test.drop('visitors', axis=1), test['visitors']\n",
    "\n",
    "pipe.fit(X_train, np.log(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Score your model on your test set.\n",
    "\n",
    "Note how your validation + test scores compared to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6074617988944077"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 80\n",
    "y_hat = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08276097481015166"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.log(y) - np.log(y_hat))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 8\n",
    "y_hat = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08276097481015166"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.log(y) - np.log(y_hat))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this case, we would report our results with a validation score of 0.589 + test scoe of 0.56.  Modest overfitting, but not too extreme.  Subsequent iterations could provide a bit more smoothing to even them out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
