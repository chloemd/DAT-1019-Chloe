{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Challenge:  Restaurant Dataset Modeling Challenge\n",
    "\n",
    "At this point you've had a chance to use a GBM on the restaurant dataset, and (hopefully) developed a feel for how different encoding techniques and model parameters can impact your overall model score.  \n",
    "\n",
    "The idea behind this bonus workbook is to give you a chance to incorporate work from previous classes into our current endeavors to see how they might fit together.  \n",
    "\n",
    "Your task, if you choose to pursue it, is to add features that were covered in classes 7 & 8 to see if they can add predictive value to your model.  \n",
    "\n",
    "To refresh your memory:\n",
    "\n",
    "**Class 7:** The use of grouping to create summary statistics that compare a value at a particular day, to the average for a larger group.  Ie, if you have attendance of 38 people on a particular day, and the restaurant averages 17 as a whole, you might have reversion to the mean.  \n",
    "\n",
    "Some columns you could go ahead and create:\n",
    "\n",
    " - average / median / std attendance for each restaurants\n",
    " - average / median / std attendance for restaurants on particular days\n",
    " - average / median / std attendance for particular genres within particular cities\n",
    " - try something on your own!\n",
    " \n",
    " The idea is that if there's a natural tendency for mean reversion then these columns would allow you to capture them.\n",
    " \n",
    "**Class 8:** In this lab we used various time shifts within our dataset -- the previous value, the value from 2 days ago, the 10, 25, 60 day moving average, different date parts, etc.  The idea here is that we want to capture as many aspects from the passage of time as possible to better inform us about what way trends are heading.\n",
    "\n",
    "Take some time to review what was done in these lessons, and try re-adding these features to your model to see how they impact your scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Hints\n",
    "\n",
    "Here are some useful tidbits to keep in mind when doing this exercise:\n",
    "\n",
    " - Lots of models begin to perform poorly when you add superfluous columns.  Tree based models are mostly immune to this.  Especially GBMs.  If a feature is unimportant, it simply won't get picked up in a boosting round.  So it's fine to add a lot and then remove them if they have no effect afterwards.\n",
    " - You only want to use values from your training set to inform your model, and want to avoid leaking values from your test set.  So if you are creating summary statistics, create training/test sets first, and use the aggregations from your training set for summary statistics on each.\n",
    " - Likewise, if you are creating shift statistics -- make sure you are not including anything from the **present day** in any of your calculations.  A good hint that you are doing this is if your model scores seem to improve by an almost impossibly large amount after adding a column.\n",
    " - Not every categorical column needs to be encoded in the same way.  Maybe Target Encoding is particularly useful for columns with lots of unique values, but one hot encoding is better for those with a smaller number of uniques.  Try experimenting with different combinations of these to find out which drives your results\n",
    " - Shift statistics create missing values.  Since these are time sensitive, the best way to fill them in is the `bfill()` method, which will take the last recorded value and backfill any missing values before that.  \n",
    " - It can be a good idea to denote missing values if you are creating lots of missing values from the previous bullet point\n",
    " - See if you can get pipelines to work for you, even if you still find them confusing\n",
    " - Some columns can be useful, but only with more powerful versions of your model.  Ie, maybe a 25 day moving average of attendance isn't *the* most important thing determining outcomes, but it might provide some inference that's unique from everything else.  Therefore, 100 boosting rounds may not be enough to detect its effect, even if it's there in some amount.  A good idea to get around this is to add a bunch of columns to your dataset at once and then fit a powerful version of your model on all of them all at once.  This will take about as much time as adding one column and then fitting to see if it improves your score.\n",
    " - With time based effects, it's often helpful to take the *difference* between a trend and the current observation.  Ie, a column with your 10 day moving average is one thing, but the difference between your current observation and the 10 day moving average might convey more usable insight with respect to what's going to happen next.  If nothing else, it can help models converge towards the correct answer faster.\n",
    "\n",
    "Feel free to experiment with ideas not listed here.  The point is to dip your toes in the water and have fun.  Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
