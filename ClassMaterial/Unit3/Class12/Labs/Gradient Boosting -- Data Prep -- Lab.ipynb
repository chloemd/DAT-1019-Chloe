{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab -- Data Prep & Gradient Boosting\n",
    "\n",
    "Welcome to today's lab!  Today we're going to shift our attention to a more demanding dataset -- the restaurants data.  A quarter million rows, dates, and categorical data make this a more interesting, realistic use case of boosting.  \n",
    "\n",
    "The point of today's lab will be to experiment with different encoding methods and model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:**  Load in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Create a training and test set.\n",
    "\n",
    "Make the test set the **last 15 observations for each restaurant**.\n",
    "\n",
    "Turn each of these variables into `X_train, y_train`, and `X_test, y_test`, respectively.\n",
    "\n",
    "**Hint:**  This harkens back to our grouping lab -- check this if you forget how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Experiment with different encoding methods\n",
    "\n",
    "Let's do a quick check to see how different encoding methods work out of the box on our dataset.\n",
    "\n",
    "You're going to repeat the same process for each of `OrdinalEncoder`, `TargetEncoder`, and `OneHotEncoder` and see which one gives you the best results on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a:** Use an `OrdinalEncoder` to transform your training set with the `fit_transform` method.  Then use the `transform` method to transform your test set.  \n",
    "\n",
    "**Important:** An important detail here is that the test set is being transformed according to the values in your training set.  \n",
    "\n",
    "If you are confused about how the transformation is happening, try using the `mapping` attribute on your category encoder to get a hang of what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b:** Initialize a `GradientBoostingRegressor` with the default parameters, fit it on your training set, and score it on your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c:** Repeat these same steps for the `TargetEncoder` and the `OneHotEncoder`\n",
    "\n",
    "**Important:** The `OneHotEncoder` can take awhile to fit.  If nothing happens in around 4 minutes, just cancel the process and try it again later on when you have more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Look at your most important features\n",
    "\n",
    "Similar to the previous lab, take your model's most important features and load them into a dataframe to see what's driving your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Can model parameters improve your score?  \n",
    "\n",
    "Take the **best** version of your encoding method and try changing some parameters with your model to see if it improves your score.  \n",
    "\n",
    "You won't have a ton of time to do this, but try some of the following:\n",
    "\n",
    " - Try increasing the number of trees your model uses -- 250, 500, or perhaps more trees if time permits\n",
    " - Try experimenting with differing values for tree depth -- the default is 3, but perhaps 4, 5 or 6 works better\n",
    " - Try improving fitting time by introducing some **randomness** into your data with the following two model parameters:\n",
    "   - `subsample`: this dictates what proportion of your data will be used for each tree.  A value of `0.7` means 70% of your data will be used for a particular tree, chosen at random\n",
    "   - `max_features`: this is the portion of columns that are used at each individual split.  If you enter an integer the model will randomly select that number of columns, if you enter a decimal it will randomly select that portion of columns.\n",
    "   - It can be very useful to find the most sparse model that will still give you comparable results.  Ie, if you find a gbm with 500 trees and a max_depth of 4 gives you the best results, it can be very beneficial if you can get those same results with a `subsample` value of 0.6 and a `max_features` score of 0.7, because your model will fit ~50% faster.\n",
    "   \n",
    "This step is open ended, so we will likely have to end class in the middle of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
